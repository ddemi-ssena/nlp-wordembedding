{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Twitter Sentiment Analysis with/without Word Embeddings**\n"
      ],
      "metadata": {
        "id": "SzS4y_6hhD71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n"
      ],
      "metadata": {
        "id": "h8aI2NfYhL0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()  # kaggle.json dosyasını seçip yükle\n"
      ],
      "metadata": {
        "id": "FfjElKY74qhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "os.rename(\"kaggle.json\", \"/root/.kaggle/kaggle.json\")\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n"
      ],
      "metadata": {
        "id": "Gi87j4jz44Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d kazanova/sentiment140\n"
      ],
      "metadata": {
        "id": "jEoQ3-7x5HSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"sentiment140.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"sentiment140\")\n"
      ],
      "metadata": {
        "id": "yd2oqJSm5JRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
      ],
      "metadata": {
        "id": "MAJeGimK62M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"sentiment140/training.1600000.processed.noemoticon.csv\", encoding='latin-1', header=None)\n",
        "df.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
        "\n",
        "df[['target', 'text']].head()\n"
      ],
      "metadata": {
        "id": "w1N7uCog5Lcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['target'] != 2]  # varsa nötrleri çıkar\n",
        "df['target'] = df['target'].replace({0: 0, 4: 1})  # 0: negatif, 1: pozitif\n"
      ],
      "metadata": {
        "id": "_sPze2qB7JS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dW9vdRfu6aYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Veri Temizleme (Preprocessing)**"
      ],
      "metadata": {
        "id": "UzQNJq397QhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def temizle(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # linkleri sil\n",
        "    text = re.sub(r'@\\w+', '', text)     # mentionları sil\n",
        "    text = re.sub(r'#\\w+', '', text)     # hashtagleri sil\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # noktalama işaretlerini sil\n",
        "    text = text.lower()                  # küçük harfe çevir\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['text'].apply(temizle)\n",
        "\n"
      ],
      "metadata": {
        "id": "xGmvtMI67VVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Eğitim ve Test Seti Ayırma**\n"
      ],
      "metadata": {
        "id": "3Ipy7U9G7cRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['clean_text']\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "URqj3RlX7bWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF Vektörleştirme (Embedding’siz çözüm)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nZU8bW5L7nIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features=5000)  # en sık geçen 5000 kelime\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "Cttha5Hu7rMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Eğitimi (Logistic Regression)**"
      ],
      "metadata": {
        "id": "DszrHiUr7tli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_tfidf, y_train)\n"
      ],
      "metadata": {
        "id": "mIphAQk07xPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tahmin ve Değerlendirme**"
      ],
      "metadata": {
        "id": "yivMCeDR71Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Doğruluk:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "kCDJXnu-76WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o45YlBkuW04T",
        "outputId": "76b2d32a-7b6d-46a6-fa8f-dc3b41bdfbbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "j1dB-H818-96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def simple_tokenizer(text):\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "df['tokens'] = df['clean_text'].apply(simple_tokenizer)\n"
      ],
      "metadata": {
        "id": "d3KToA9wwdGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec modeli eğit**"
      ],
      "metadata": {
        "id": "ZDZRfTDP_fin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n"
      ],
      "metadata": {
        "id": "K-FTW9p-_iTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokens sütununu liste listesine çeviriyoruz\n",
        "sentences = df['tokens'].tolist()\n",
        "\n",
        "# Word2Vec modelini eğitiyoruz\n",
        "w2v_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=2, workers=4)\n"
      ],
      "metadata": {
        "id": "e_S90bfXx2rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_avg_vector(tokens, model, vector_size):\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(vector_size)\n",
        "    else:\n",
        "        return np.mean(vectors, axis=0)\n",
        "\n",
        "vector_size = 100\n",
        "df['vector'] = df['tokens'].apply(lambda x: get_avg_vector(x, w2v_model, vector_size))\n"
      ],
      "metadata": {
        "id": "1zqcz3Xfx6-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sınıflandırma Modeli Kur (örneğin Logistic Regression)**"
      ],
      "metadata": {
        "id": "NpH8ojfCx9r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X = list(df['vector'])\n",
        "y = df['target']  # ya da df['label'] hangisiyse\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "vtB5E9-ux85a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Word2Vec modelinden alınan vektörler\n",
        "model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Vektörleri kaydet\n",
        "words = list(model.wv.index_to_key)\n",
        "vectors = model.wv[words]\n",
        "\n",
        "# Vektörleri bir dosyaya kaydedelim (word2vec formatında)\n",
        "with open('vectors.tsv', 'w') as f:\n",
        "    for word, vector in zip(words, vectors):\n",
        "        f.write(f\"{word}\\t\" + \"\\t\".join(map(str, vector)) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "ndeaFcYi2p6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('vectors.tsv')\n"
      ],
      "metadata": {
        "id": "kkr3QSwJ3r_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metadata dosyasını oluştur\n",
        "with open('metadata.tsv', 'w') as f:\n",
        "    for word in words:\n",
        "        f.write(f\"{word}\\n\")\n"
      ],
      "metadata": {
        "id": "iqNf73vu7SgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('metadata.tsv')"
      ],
      "metadata": {
        "id": "WgmDhl9J70k2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
